{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "583684ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"alice.txt\") as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c04848c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145178"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dd79558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn,optim\n",
    "from torch.nn import functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d49cb726",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(set(text))\n",
    "indexer = {char: index for (index, char) in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db04f557",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_enc = [indexer[ch] for ch in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bef40c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toOneHot(batch):\n",
    "    batch_flat = batch.flatten()\n",
    "    oneHot_flat = np.zeros((len(batch_flat), len(chars)))\n",
    "    oneHot_flat[range(len(batch_flat)), batch_flat] = 1\n",
    "    oneHot = oneHot_flat.reshape(batch.shape[0], batch.shape[1], -1)\n",
    "    return oneHot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b7f004db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[32, 24,  3, ..., 33, 56, 39],\n",
       "       [22, 65, 29, ..., 38, 19, 18],\n",
       "       [ 3, 18, 33, ..., 64, 43, 18],\n",
       "       ...,\n",
       "       [48, 33, 51, ..., 29, 27, 18],\n",
       "       [30, 22,  1, ..., 64, 33,  1],\n",
       "       [38, 51, 33, ..., 51, 65, 65]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "text_seq = np.array(text_enc)[:(len(text)//100 * 100)].reshape(batch_size,-1)\n",
    "text_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a644001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, char_len, hidden_size, n_layers):\n",
    "        super().__init__();\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm  = nn.LSTM(char_len,hidden_size, n_layers, batch_first = True)\n",
    "        self.output = nn.Linear(hidden_size, char_len)\n",
    "    def forward(self, x, states):\n",
    "        out, states = self.lstm(x, states)\n",
    "        out = out.clone().contiguous().view(-1, self.hidden_size)\n",
    "        out = self.output(out)\n",
    "        return out,states\n",
    "    def init_states(self, batch_size):\n",
    "        hidden = next(self.parameters()).data.new(self.n_layers, batch_size, self.hidden_size).zero_()\n",
    "        cell = next(self.parameters()).data.new(self.n_layers, batch_size, self.hidden_size).zero_()\n",
    "        states = (hidden, cell)\n",
    "        return states "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c896f4ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "137bfdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 50\n",
    "batch_size = 100\n",
    "model = LSTM(len(chars), 256, 2).to(\"cuda\")\n",
    "epochs = 500\n",
    "optimizer = optim.Adam(model.parameters(),  lr = 0.001)\n",
    "loss_funct = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d1c3be46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 96.19911479949951\n",
      "2 96.15578126907349\n",
      "3 96.02055621147156\n",
      "4 95.8379979133606\n",
      "5 95.26189231872559\n",
      "6 92.93992853164673\n",
      "7 87.88155674934387\n",
      "8 83.18486881256104\n",
      "9 79.66957521438599\n",
      "10 77.1502947807312\n",
      "11 75.15795707702637\n",
      "12 73.49986672401428\n",
      "13 72.27524590492249\n",
      "14 70.94341778755188\n",
      "15 69.75299406051636\n",
      "16 68.52092337608337\n",
      "17 67.50317811965942\n",
      "18 66.29314184188843\n",
      "19 65.30277609825134\n",
      "20 64.2870991230011\n",
      "21 63.34780144691467\n",
      "22 62.453773856163025\n",
      "23 61.57707703113556\n",
      "24 60.79306495189667\n",
      "25 59.98186707496643\n",
      "26 59.162375926971436\n",
      "27 58.384562730789185\n",
      "28 57.68370747566223\n",
      "29 56.980650424957275\n",
      "30 56.24401533603668\n",
      "31 55.517231464385986\n",
      "32 54.851921796798706\n",
      "33 54.22270846366882\n",
      "34 53.555848360061646\n",
      "35 52.998873233795166\n",
      "36 52.28556740283966\n",
      "37 51.64685344696045\n",
      "38 51.013728618621826\n",
      "39 50.44241213798523\n",
      "40 49.914626121520996\n",
      "41 49.3010174036026\n",
      "42 48.58880913257599\n",
      "43 48.14008319377899\n",
      "44 47.636850357055664\n",
      "45 46.90933549404144\n",
      "46 46.36882996559143\n",
      "47 45.905805230140686\n",
      "48 45.635165214538574\n",
      "49 44.918137311935425\n",
      "50 44.327104568481445\n",
      "51 43.743812680244446\n",
      "52 43.21957719326019\n",
      "53 42.73107886314392\n",
      "54 42.18067407608032\n",
      "55 41.607741951942444\n",
      "56 41.112624645233154\n",
      "57 40.70480442047119\n",
      "58 40.30130064487457\n",
      "59 39.841925501823425\n",
      "60 39.42923283576965\n",
      "61 38.838961482048035\n",
      "62 38.46641671657562\n",
      "63 38.28381407260895\n",
      "64 37.45326364040375\n",
      "65 36.62708556652069\n",
      "66 36.056971311569214\n",
      "67 35.51014304161072\n",
      "68 35.125967383384705\n",
      "69 34.70611393451691\n",
      "70 34.11395740509033\n",
      "71 33.61006933450699\n",
      "72 33.22140955924988\n",
      "73 32.60176998376846\n",
      "74 32.231883227825165\n",
      "75 32.11369699239731\n",
      "76 31.45835542678833\n",
      "77 30.97938174009323\n",
      "78 30.650050699710846\n",
      "79 29.911937654018402\n",
      "80 29.360239326953888\n",
      "81 28.894762217998505\n",
      "82 28.478368997573853\n",
      "83 27.937973707914352\n",
      "84 27.356209456920624\n",
      "85 27.07643136382103\n",
      "86 26.823025733232498\n",
      "87 26.279832392930984\n",
      "88 25.785338521003723\n",
      "89 25.403534471988678\n",
      "90 25.218158036470413\n",
      "91 25.101139962673187\n",
      "92 24.728708654642105\n",
      "93 23.928287029266357\n",
      "94 23.215173333883286\n",
      "95 22.808363050222397\n",
      "96 22.548952877521515\n",
      "97 22.089826732873917\n",
      "98 21.58079919219017\n",
      "99 21.09219354391098\n",
      "100 20.744628995656967\n",
      "101 20.62775829434395\n",
      "102 20.2601298391819\n",
      "103 19.94164076447487\n",
      "104 19.53984270989895\n",
      "105 19.002960324287415\n",
      "106 18.560686260461807\n",
      "107 18.049008563160896\n",
      "108 17.42661675810814\n",
      "109 17.1141357421875\n",
      "110 16.947092831134796\n",
      "111 16.734543845057487\n",
      "112 16.678199857473373\n",
      "113 16.500347405672073\n",
      "114 16.072786331176758\n",
      "115 15.51205475628376\n",
      "116 14.998319268226624\n",
      "117 14.550592757761478\n",
      "118 14.158773452043533\n",
      "119 13.90411664545536\n",
      "120 13.691509306430817\n",
      "121 13.314398653805256\n",
      "122 12.979775205254555\n",
      "123 12.730827987194061\n",
      "124 12.497675567865372\n",
      "125 12.234718263149261\n",
      "126 12.046985246241093\n",
      "127 11.733780823647976\n",
      "128 11.411949381232262\n",
      "129 11.27227558940649\n",
      "130 10.859836101531982\n",
      "131 10.68432653695345\n",
      "132 10.35638227313757\n",
      "133 10.026945941150188\n",
      "134 9.833827264606953\n",
      "135 9.620142102241516\n",
      "136 9.422668062150478\n",
      "137 9.38813740760088\n",
      "138 9.324088528752327\n",
      "139 8.955185487866402\n",
      "140 8.668610036373138\n",
      "141 8.366440050303936\n",
      "142 8.11156751960516\n",
      "143 7.9112439006567\n",
      "144 7.701155476272106\n",
      "145 7.455053202807903\n",
      "146 7.195219069719315\n",
      "147 6.912100315093994\n",
      "148 6.566453903913498\n",
      "149 6.3321506679058075\n",
      "150 6.2115340530872345\n",
      "151 6.029168501496315\n",
      "152 6.032909356057644\n",
      "153 5.967870645225048\n",
      "154 5.877664469182491\n",
      "155 5.8461799249053\n",
      "156 5.753922779113054\n",
      "157 5.458743955940008\n",
      "158 5.275621246546507\n",
      "159 5.074254993349314\n",
      "160 4.939025640487671\n",
      "161 4.738879647105932\n",
      "162 4.575600266456604\n",
      "163 4.481394551694393\n",
      "164 4.365712888538837\n",
      "165 4.181063584983349\n",
      "166 4.027887806296349\n",
      "167 3.850133992731571\n",
      "168 3.7588064409792423\n",
      "169 3.643806543201208\n",
      "170 3.5424003079533577\n",
      "171 3.473647329956293\n",
      "172 3.347990293055773\n",
      "173 3.2142861671745777\n",
      "174 3.1903361678123474\n",
      "175 3.1094539500772953\n",
      "176 3.1052418686449528\n",
      "177 3.0395250469446182\n",
      "178 3.002626221626997\n",
      "179 2.856051130220294\n",
      "180 2.881870325654745\n",
      "181 2.7918097097426653\n",
      "182 2.7340203020721674\n",
      "183 2.596131993457675\n",
      "184 2.6245865542441607\n",
      "185 2.6004272159188986\n",
      "186 2.5788455680012703\n",
      "187 2.5287621282041073\n",
      "188 2.4926258474588394\n",
      "189 2.3100307174026966\n",
      "190 2.2731581069529057\n",
      "191 2.2207356076687574\n",
      "192 2.1389459278434515\n",
      "193 2.0609681513160467\n",
      "194 2.0752105843275785\n",
      "195 2.095151722431183\n",
      "196 2.0552269406616688\n",
      "197 2.002093840390444\n",
      "198 1.950645787641406\n",
      "199 1.8494695015251637\n",
      "200 1.7842371594160795\n",
      "201 1.7142569087445736\n",
      "202 1.6234514638781548\n",
      "203 1.575088832527399\n",
      "204 1.5129148550331593\n",
      "205 1.4685721322894096\n",
      "206 1.3757528588175774\n",
      "207 1.310469139367342\n",
      "208 1.206699843518436\n",
      "209 1.145636873319745\n",
      "210 1.0771553330123425\n",
      "211 1.0746737653389573\n",
      "212 1.0410682614892721\n",
      "213 0.9977663354948163\n",
      "214 0.9856883566826582\n",
      "215 0.9839939521625638\n",
      "216 0.9601525459438562\n",
      "217 0.9125974113121629\n",
      "218 0.8933746190741658\n",
      "219 0.8592404071241617\n",
      "220 0.8203033283352852\n",
      "221 0.7740854453295469\n",
      "222 0.7553159762173891\n",
      "223 0.7266255337744951\n",
      "224 0.7041270509362221\n",
      "225 0.6725606736727059\n",
      "226 0.6567243649624288\n",
      "227 0.6332352729514241\n",
      "228 0.5955559825524688\n",
      "229 0.5730826733633876\n",
      "230 0.5466034854762256\n",
      "231 0.5211717626079917\n",
      "232 0.4989506248384714\n",
      "233 0.4778015948832035\n",
      "234 0.4462885167449713\n",
      "235 0.4137676637619734\n",
      "236 0.3812024728395045\n",
      "237 0.3508257307112217\n",
      "238 0.32898852601647377\n",
      "239 0.31231950828805566\n",
      "240 0.29626890923827887\n",
      "241 0.28970258263871074\n",
      "242 0.28567405650392175\n",
      "243 0.2650759683456272\n",
      "244 0.2538145480211824\n",
      "245 0.24649039702489972\n",
      "246 0.2353689989540726\n",
      "247 0.22671258845366538\n",
      "248 0.22012414829805493\n",
      "249 0.21346705104224384\n",
      "250 0.21177465189248323\n",
      "251 0.21378562902100384\n",
      "252 0.20803023711778224\n",
      "253 0.20330880559049547\n",
      "254 0.19954713061451912\n",
      "255 0.19061869848519564\n",
      "256 0.18710608407855034\n",
      "257 0.18141935346648097\n",
      "258 0.1800678565632552\n",
      "259 0.1754937432706356\n",
      "260 0.17302001267671585\n",
      "261 0.1681999135762453\n",
      "262 0.16740664467215538\n",
      "263 0.16327363741584122\n",
      "264 0.16180143249221146\n",
      "265 0.1579133451450616\n",
      "266 0.15703107276931405\n",
      "267 0.15344439423643053\n",
      "268 0.15296831727027893\n",
      "269 0.15348548698239028\n",
      "270 0.15796406846493483\n",
      "271 0.15871836827136576\n",
      "272 0.1518557135714218\n",
      "273 0.1465632023755461\n",
      "274 0.14613547339104116\n",
      "275 0.14365731016732752\n",
      "276 0.142312983982265\n",
      "277 0.14012690668459982\n",
      "278 0.14205683884210885\n",
      "279 0.1430133138783276\n",
      "280 0.14421058527659625\n",
      "281 0.1794949050527066\n",
      "282 0.4144380711950362\n",
      "283 8.1486259624362\n",
      "284 19.73130676150322\n",
      "285 12.314567387104034\n",
      "286 6.551761711016297\n",
      "287 3.7645519375801086\n",
      "288 2.283386167138815\n",
      "289 1.4811528166756034\n",
      "290 0.9821145758032799\n",
      "291 0.697914285119623\n",
      "292 0.5488659935072064\n",
      "293 0.47451466461643577\n",
      "294 0.424436429515481\n",
      "295 0.3860981003381312\n",
      "296 0.35664867889136076\n",
      "297 0.33137537632137537\n",
      "298 0.31024278444238007\n",
      "299 0.29341653268784285\n",
      "300 0.2785004337783903\n",
      "301 0.2661042311228812\n",
      "302 0.25476319226436317\n",
      "303 0.24514431902207434\n",
      "304 0.23590538348071277\n",
      "305 0.22856064280495048\n",
      "306 0.22095231665298343\n",
      "307 0.21514733228832483\n",
      "308 0.20856714504770935\n",
      "309 0.20304731419309974\n",
      "310 0.1968496972694993\n",
      "311 0.1923672838602215\n",
      "312 0.186889395583421\n",
      "313 0.18330068862996995\n",
      "314 0.17845444194972515\n",
      "315 0.17503617983311415\n",
      "316 0.1706277864286676\n",
      "317 0.16806688159704208\n",
      "318 0.16438971366733313\n",
      "319 0.1621579600032419\n",
      "320 0.15862563124392182\n",
      "321 0.15565583552233875\n",
      "322 0.15193413919769228\n",
      "323 0.15390461310744286\n",
      "324 0.15384353778790683\n",
      "325 0.14870116370730102\n",
      "326 0.1447539790533483\n",
      "327 0.14411950204521418\n",
      "328 0.1419622249668464\n",
      "329 0.13836675719358027\n",
      "330 0.1344551236834377\n",
      "331 0.13360524433664978\n",
      "332 0.1304695118451491\n",
      "333 0.1303514780011028\n",
      "334 0.12682791787665337\n",
      "335 0.1288409905973822\n",
      "336 0.12768708600196987\n",
      "337 0.12565172370523214\n",
      "338 0.12200309918262064\n",
      "339 0.12008543242700398\n",
      "340 0.11794074939098209\n",
      "341 0.12227097316645086\n",
      "342 0.12367457686923444\n",
      "343 0.12369923398364335\n",
      "344 0.13206703192554414\n",
      "345 0.16645692416932434\n",
      "346 0.19539667142089456\n",
      "347 0.2262300185393542\n",
      "348 0.3228740864433348\n",
      "349 0.5553455529734492\n",
      "350 2.175607804208994\n",
      "351 8.448273055255413\n",
      "352 10.407028287649155\n",
      "353 6.966693088412285\n",
      "354 3.828440331853926\n",
      "355 2.015344640240073\n",
      "356 1.0716720190830529\n",
      "357 0.6216680132783949\n",
      "358 0.4126561554148793\n",
      "359 0.32908342964947224\n",
      "360 0.27753900247626007\n",
      "361 0.24638833315111697\n",
      "362 0.22644609212875366\n",
      "363 0.21123278210870922\n",
      "364 0.20148444641381502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365 0.19203219143673778\n",
      "366 0.1846058696974069\n",
      "367 0.17571164621040225\n",
      "368 0.17314602667465806\n",
      "369 0.16869495576247573\n",
      "370 0.16164210729766637\n",
      "371 0.16086007072590292\n",
      "372 0.15897930227220058\n",
      "373 0.1522150725359097\n",
      "374 0.14543754479382187\n",
      "375 0.14046132017392665\n",
      "376 0.13668901240453124\n",
      "377 0.13402553333435208\n",
      "378 0.13006262050475925\n",
      "379 0.1277496423572302\n",
      "380 0.12526462553068995\n",
      "381 0.12382270151283592\n",
      "382 0.1204227446578443\n",
      "383 0.12025246326811612\n",
      "384 0.11741758068092167\n",
      "385 0.1287961604539305\n",
      "386 0.1282022978994064\n",
      "387 0.12128876615315676\n",
      "388 0.11472142988350242\n",
      "389 0.11088114039739594\n",
      "390 0.10877395886927843\n",
      "391 0.11037003644742072\n",
      "392 0.10745563334785402\n",
      "393 0.10590660205343738\n",
      "394 0.10237309767398983\n",
      "395 0.1006964729167521\n",
      "396 0.09838964138180017\n",
      "397 0.09779697470366955\n",
      "398 0.095458296360448\n",
      "399 0.09473382856231183\n",
      "400 0.09324422955978662\n",
      "401 0.09324005816597492\n",
      "402 0.09260698047000915\n",
      "403 0.11495242430828512\n",
      "404 0.1337339497404173\n",
      "405 0.12736613542074338\n",
      "406 0.11729642609134316\n",
      "407 0.10163453425047919\n",
      "408 0.09470301651163027\n",
      "409 0.09145405504386872\n",
      "410 0.08928155608009547\n",
      "411 0.088432478136383\n",
      "412 0.08853311464190483\n",
      "413 0.0860612973337993\n",
      "414 0.0859401726629585\n",
      "415 0.08845189947169274\n",
      "416 0.08291599771473557\n",
      "417 0.08108366455417126\n",
      "418 0.0806367980549112\n",
      "419 0.07938178960466757\n",
      "420 0.0817617675056681\n",
      "421 0.08229068823857233\n",
      "422 0.08470479305833578\n",
      "423 0.08303577109472826\n",
      "424 0.08475026662927121\n",
      "425 0.08484003460034728\n",
      "426 0.11672445898875594\n",
      "427 0.11336765595478937\n",
      "428 0.10843031702097505\n",
      "429 0.12053349387133494\n",
      "430 0.1550034296233207\n",
      "431 0.361263909842819\n",
      "432 3.520495491102338\n",
      "433 14.029633581638336\n",
      "434 12.484150886535645\n",
      "435 6.682219936512411\n",
      "436 3.431462694425136\n",
      "437 1.859441976994276\n",
      "438 1.0240545189008117\n",
      "439 0.5957992819603533\n",
      "440 0.38562926929444075\n",
      "441 0.30046668089926243\n",
      "442 0.25362707674503326\n",
      "443 0.22350871586240828\n",
      "444 0.20570852421224117\n",
      "445 0.19120424916036427\n",
      "446 0.1808383318129927\n",
      "447 0.17232465115375817\n",
      "448 0.16463862522505224\n",
      "449 0.15848225820809603\n",
      "450 0.1526720984838903\n",
      "451 0.14859930472448468\n",
      "452 0.14281783858314157\n",
      "453 0.13881412451155484\n",
      "454 0.13469436089508235\n",
      "455 0.13148621120490134\n",
      "456 0.12788710196036845\n",
      "457 0.12509081151802093\n",
      "458 0.12205924780573696\n",
      "459 0.11974063701927662\n",
      "460 0.11705936572980136\n",
      "461 0.11506172525696456\n",
      "462 0.11275853321421891\n",
      "463 0.11120341741479933\n",
      "464 0.10904277203371748\n",
      "465 0.10795140010304749\n",
      "466 0.10560882691061124\n",
      "467 0.10534437256865203\n",
      "468 0.10266287421109155\n",
      "469 0.10267272125929594\n",
      "470 0.0999637697241269\n",
      "471 0.09992820932529867\n",
      "472 0.0973555181408301\n",
      "473 0.0974389387993142\n",
      "474 0.09512114757671952\n",
      "475 0.09640753176063299\n",
      "476 0.09409342508297414\n",
      "477 0.09371635713614523\n",
      "478 0.09092268138192594\n",
      "479 0.08936398173682392\n",
      "480 0.08767302706837654\n",
      "481 0.08634207816794515\n",
      "482 0.08515451475977898\n",
      "483 0.084042118571233\n",
      "484 0.08329620450967923\n",
      "485 0.08230958133935928\n",
      "486 0.08114939887309447\n",
      "487 0.08011535269906744\n",
      "488 0.07932078640442342\n",
      "489 0.07839833316393197\n",
      "490 0.07738339324714616\n",
      "491 0.07652933086501434\n",
      "492 0.07580110320122913\n",
      "493 0.07526332308771089\n",
      "494 0.07452384987846017\n",
      "495 0.07346429431345314\n",
      "496 0.07304314064094797\n",
      "497 0.07315718510653824\n",
      "498 0.07213692035293207\n",
      "499 0.07127188588492572\n",
      "500 0.07126612670253962\n"
     ]
    }
   ],
   "source": [
    "loss_arr = []\n",
    "for e in range(1, epochs + 1):\n",
    "    loss_cur = 0;\n",
    "    states = model.init_states(batch_size)\n",
    "    for b in range(0, text_seq.shape[1], seq_len):\n",
    "        x_batch = text_seq[:, b:b+seq_len]\n",
    "        if b == text_seq.shape[1]-seq_len:\n",
    "            y_batch = text_seq[:, b+1:b+seq_len]\n",
    "            y_batch = np.hstack((y_batch, indexer[\".\"] *np.ones((y_batch.shape[0]))))\n",
    "        elif b + 1 == text_seq.shape[1]:\n",
    "            y_batch = indexer[\".\"] * np.ones((y_batch.shape[0]))\n",
    "        else:\n",
    "            y_batch = text_seq[:, b+1:b+seq_len+1]\n",
    "#             print(b)\n",
    "    \n",
    "        x = torch.Tensor(toOneHot(x_batch)).to(\"cuda\")\n",
    "        y = torch.Tensor(y_batch).view(-1).to(\"cuda\")\n",
    "        pred, states = model(x, states)\n",
    "        states = tuple([each.data for each in states])\n",
    "#         print(pred.shape, y.shape)\n",
    "        loss = loss_funct(pred, y.long())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph = True)\n",
    "        optimizer.step()\n",
    "        loss_cur+=loss.item()\n",
    "\n",
    "    print(e, loss_cur)\n",
    "    loss_arr.append(loss_cur)\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d127d9d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1451)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b1dd1f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So she was considering in her own mind us on, \n",
      "So she was considering in her own mind us on, t\n",
      "So she was considering in her own mind us on, th\n",
      "So she was considering in her own mind us on, tho\n",
      "So she was considering in her own mind us on, thou\n",
      "So she was considering in her own mind us on, thoug\n",
      "So she was considering in her own mind us on, though\n",
      "So she was considering in her own mind us on, though \n",
      "So she was considering in her own mind us on, though t\n",
      "So she was considering in her own mind us on, though th\n",
      "So she was considering in her own mind us on, though the\n",
      "So she was considering in her own mind us on, though the\n",
      "\n",
      "So she was considering in her own mind us on, though the\n",
      "L\n",
      "So she was considering in her own mind us on, though the\n",
      "Lo\n",
      "So she was considering in her own mind us on, though the\n",
      "Lor\n",
      "So she was considering in her own mind us on, though the\n",
      "Lory\n",
      "So she was considering in her own mind us on, though the\n",
      "Lory \n",
      "So she was considering in her own mind us on, though the\n",
      "Lory h\n",
      "So she was considering in her own mind us on, though the\n",
      "Lory ha\n",
      "So she was considering in her own mind us on, though the\n",
      "Lory han\n",
      "So she was considering in her own mind us on, though the\n",
      "Lory hann\n",
      "So she was considering in her own mind us on, though the\n",
      "Lory hanni\n",
      "So she was considering in her own mind us on, though the\n",
      "Lory hannin\n",
      "So she was considering in her own mind us on, though the\n",
      "Lory hanning\n",
      "So she was considering in her own mind us on, though the\n",
      "Lory hanning \n",
      "So she was considering in her own mind us on, though the\n",
      "Lory hanning i\n",
      "So she was considering in her own mind us on, though the\n",
      "Lory hanning in\n",
      "So she was considering in her own mind us on, though the\n",
      "Lory hanning in \n",
      "So she was considering in her own mind us on, though the\n",
      "Lory hanning in i\n",
      "So she was considering in her own mind us on, though the\n",
      "Lory hanning in it\n",
      "So she was considering in her own mind us on, though the\n",
      "Lory hanning in it)\n",
      "So she was considering in her own mind us on, though the\n",
      "Lory hanning in it)\n",
      "\n",
      "So she was considering in her own mind us on, though the\n",
      "Lory hanning in it)\n",
      "\n",
      "\n",
      "So she was considering in her own mind us on, though the\n",
      "Lory hanning in it)\n",
      "\n",
      "\"\n",
      "So she was considering in her own mind us on, though the\n",
      "Lory hanning in it)\n",
      "\n",
      "\"I\n",
      "So she was considering in her own mind us on, though the\n",
      "Lory hanning in it)\n",
      "\n",
      "\"I \n",
      "So she was considering in her own mind us on, though the\n",
      "Lory hanning in it)\n",
      "\n",
      "\"I c\n",
      "So she was considering in her own mind us on, though the\n",
      "Lory hanning in it)\n",
      "\n",
      "\"I ca\n",
      "So she was considering in her own mind us on, though the\n",
      "Lory hanning in it)\n",
      "\n",
      "\"I can\n",
      "So she was considering in her own mind us on, though the\n",
      "Lory hanning in it)\n",
      "\n",
      "\"I can'\n",
      "So she was considering in her own mind us on, though the\n",
      "Lory hanning in it)\n",
      "\n",
      "\"I can't\n",
      "So she was considering in her own mind us on, though the\n",
      "Lory hanning in it)\n",
      "\n",
      "\"I can't \n",
      "So she was considering in her own mind us on, though the\n",
      "Lory hanning in it)\n",
      "\n",
      "\"I can't r\n",
      "So she was considering in her own mind us on, though the\n",
      "Lory hanning in it)\n",
      "\n",
      "\"I can't re\n",
      "So she was considering in her own mind us on, though the\n",
      "Lory hanning in it)\n",
      "\n",
      "\"I can't rem\n",
      "So she was considering in her own mind us on, though the\n",
      "Lory hanning in it)\n",
      "\n",
      "\"I can't reme\n",
      "So she was considering in her own mind us on, though the\n",
      "Lory hanning in it)\n",
      "\n",
      "\"I can't remem\n",
      "So she was considering in her own mind us on, though the\n",
      "Lory hanning in it)\n",
      "\n",
      "\"I can't rememb\n",
      "So she was considering in her own mind us on, though the\n",
      "Lory hanning in it)\n",
      "\n",
      "\"I can't remembe\n",
      "So she was considering in her own mind us on, though the\n",
      "Lory hanning in it)\n",
      "\n",
      "\"I can't remember\n"
     ]
    }
   ],
   "source": [
    "starter = \"So she was considering in her own mind us on,\"\n",
    "states = None\n",
    "model.eval()\n",
    "model.to(\"cpu\")\n",
    "for ch in starter:\n",
    "    x = np.array([[indexer[ch]]])\n",
    "    x = toOneHot(x)\n",
    "    x = torch.Tensor(x)\n",
    "    \n",
    "    pred, states = model(x, states)\n",
    "\n",
    "counter = 0\n",
    "while starter[-1] != \".\" and counter < 50:\n",
    "    counter+=1\n",
    "    x = np.array([[indexer[starter[-1]]]])\n",
    "    x = toOneHot(x)\n",
    "    x = torch.Tensor(x).to(\"cpu\")\n",
    "    \n",
    "    pred,  states = model(x, states)\n",
    "    pred = F.softmax(pred,  dim  = 1)\n",
    "    p, top = pred.topk(10)\n",
    "    p = p.detach().numpy()[0]\n",
    "    top  = top.numpy()[0]\n",
    "    index =  np.random.choice(top, p = p/p.sum())\n",
    "    \n",
    "    starter += chars[index]\n",
    "    print(starter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbab22d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
